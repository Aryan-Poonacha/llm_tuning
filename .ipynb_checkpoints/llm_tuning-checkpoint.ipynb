{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99f99a4-1570-467e-89e1-5756799f34d1",
   "metadata": {},
   "source": [
    "## Finetuning Superlightweight Gemma-2B and Comparing Baseline Vs. Finetuned For Humor Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365c0ea-b22d-48a5-9289-0227ae3c7b4c",
   "metadata": {},
   "source": [
    "### 1. Setting Up The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a58204-1a1d-4db6-bcb8-cdbf40791fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cd569fc-07d6-4146-b950-e4993455236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e38ea54-9b75-45b3-bd1c-3547bc3858cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f010403e-2901-495a-9bb1-e699c183d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_complete = pd.read_csv('Data/humour/dataset.csv')\n",
    "\n",
    "np.random.seed(12345)  # setting random seed for reproducibility\n",
    "df = df_complete.sample(n=256)\n",
    "#df = df_complete.head(n=200)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1eddc8a-e030-47ea-a11c-cdfeb1f9242b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and the model for 2B\n",
    "tokenizer_2b = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model_2b = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", token = access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e0d4e8-ceb2-4cf4-a3b5-a456a2a7d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_string(input_string):\n",
    "    # Split the string into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', input_string)\n",
    "\n",
    "    # Reverse the list to start from the end of the string\n",
    "    sentences.reverse()\n",
    "    count = 0\n",
    "    # Iterate over each sentence\n",
    "    for sentence in sentences:\n",
    "        if count > 2:\n",
    "            break\n",
    "        # Count the occurrences of 'TRUE' and 'FALSE'\n",
    "        true_count = sentence.count('TRUE')\n",
    "        false_count = sentence.count('FALSE')\n",
    "\n",
    "        # Check the counts and return the appropriate value\n",
    "        if true_count > 0 and false_count == 0:\n",
    "            return 'TRUE'\n",
    "        elif false_count > 0 and true_count == 0:\n",
    "            return 'FALSE'\n",
    "        count+=1\n",
    "\n",
    "    # If no 'TRUE' or 'FALSE' is found in the last three sentences, check the whole string\n",
    "    true_count = input_string.count('TRUE')\n",
    "    false_count = input_string.count('FALSE')\n",
    "\n",
    "    if true_count > 0 and false_count == 0:\n",
    "        return 'TRUE'\n",
    "    elif false_count > 0 and true_count == 0:\n",
    "        return 'FALSE'\n",
    "\n",
    "    # If both or none are present, return None\n",
    "    return None\n",
    "        \n",
    "# Function to prompt gemma to classify humor with provided text\n",
    "def classify_humor_2b(text):\n",
    "    prompt = 'Please detect whether or not the following text within square brackets contains any kind of humor. Return \\'TRUE\\' in plaintext if it does and \\'FALSE\\' if it does not : [' + text + ']' \n",
    "    input_ids = tokenizer_2b.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model_2b.generate(input_ids, max_new_tokens = 512)\n",
    "    decoded_output = tokenizer_2b.decode(outputs[0])\n",
    "    print(decoded_output)\n",
    "    return check_string(decoded_output.replace(prompt, '').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71c2132-3107-4b00-a7d4-4f0f03663061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's more effective than an islamic call to prayer? a rape whistle.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_joke1 = str(test_df['text'].iloc[4])\n",
    "test_humor1 = str(test_df['humor'].iloc[4])\n",
    "print(test_joke1)\n",
    "print(test_humor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e45f408-1533-4d34-98aa-29712a48c560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Please detect whether or not the following text within square brackets contains any kind of humor. Return 'TRUE' in plaintext if it does and 'FALSE' if it does not : [What's more effective than an islamic call to prayer? a rape whistle.]\n",
      "\n",
      "Answer:\n",
      "\n",
      "Step 1/2\n",
      "First, we need to check if the text contains any kind of humor. To do this, we can use a regular expression to search for any kind of humor, such as a pun, a play on words, or a joke. Here's a Python code snippet that uses the re module to search for any kind of humor in the text: ```python import re text = \"[What's more effective than an islamic call to prayer? a rape whistle.]\" humor_regex = re.compile(r\"(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w+)\\s*(\\w\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test the baseline model on the test set\n",
    "result = classify_humor_2b(test_joke1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c63745-27a7-4fd5-a7ee-9ba138303b16",
   "metadata": {},
   "source": [
    "### Next, let's run the baseline model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1e216-b577-431d-b0f6-7fa394fac968",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to classify humor for all text in the test set\n",
    "def classify_all_humor_2b(df):\n",
    "    df['predicted_humor'] = df['text'].apply(classify_humor_2b)\n",
    "    return df\n",
    "\n",
    "def process_and_save(df):\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Apply the function to the test set\n",
    "    df_copy = classify_all_humor_2b(df_copy)\n",
    "\n",
    "    # Convert the 'humor' and 'predicted_humor' columns to boolean\n",
    "    df_copy['humor'] = df_copy['humor'].apply(lambda x: True if x == 'TRUE' else False)\n",
    "    df_copy['predicted_humor'] = df_copy['predicted_humor'].apply(lambda x: True if x == 'TRUE' else False)\n",
    "\n",
    "    # Print the F1 score, confusion matrix, and other relevant classification metrics\n",
    "    classification_report_str = classification_report(df_copy['humor'], df_copy['predicted_humor'])\n",
    "\n",
    "    # Save the classification report\n",
    "    with open('classification_report.txt', 'w') as f:\n",
    "        f.write(classification_report_str)\n",
    "\n",
    "    # Generate and save the confusion matrix\n",
    "    confusion_mat = confusion_matrix(df_copy['humor'], df_copy['predicted_humor'])\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(confusion_mat, annot=True)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "\n",
    "# Call the function with your dataframe\n",
    "process_and_save(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d87489-c982-42a4-b9df-e01ec1ed4b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"Text: {example['text'][0]}\\nHumor: {example['humor'][0]}\"\n",
    "    return [text]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "    \n",
    "trainer_2b = SFTTrainer(\n",
    "    model=model_2b,\n",
    "    train_dataset=train_set,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5109cbe-74ae-43f4-8ff9-1d7172e71ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_2b.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049efc61-f968-4a2a-9799-28586eba0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer_2b.model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f155806-79c7-45cd-8b5f-fdc52828963b",
   "metadata": {},
   "source": [
    "### Next, testing the finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f33de9a-ff26-47bc-9000-b0d1f60d0109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "#load finetuned model\n",
    "tunedmodel_2b = AutoModelForCausalLM.from_pretrained(\"trained-model\", device_map=\"auto\", token = access_token)\n",
    "tunedmodel_2b.to(\"cuda\")\n",
    "\n",
    "# Function to prompt gemma to classify humor with provided text\n",
    "def classify_humor_tuned2b(text):\n",
    "    prompt = 'Please detect whether or not the following text within square brackets contains any kind of humor. Return \\'TRUE\\' in plaintext if it does and \\'FALSE\\' if it does not : [' + text + ']' \n",
    "    input_ids = tokenizer_2b.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = tunedmodel_2b.generate(input_ids, max_new_tokens = 512)\n",
    "    decoded_output = tokenizer_2b.decode(outputs[0])\n",
    "    print(decoded_output)\n",
    "    return check_string(decoded_output.replace(prompt, '').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56526ea-f394-435f-a041-94344209f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Please detect whether or not the following text within square brackets contains any kind of humor. Return 'TRUE' in plaintext if it does and 'FALSE' if it does not : [What's more effective than an islamic call to prayer? a rape whistle.]\n",
      "\n",
      "Answer:\n",
      "\n",
      "Step 1/2\n",
      "First, we need to check if the text contains any kind of humor. To do this, we can use a regular expression to search for any kind of humor, such as a pun, a play on words, or a sarcastic tone. Here's a Python code snippet that uses the re module to search for humor in the given text: ```python import re text = \"[What's more effective than an islamic call to prayer? a rape whistle.]\" humor_regex = re.compile(r\"(\\w+) (\\w+)\") humor_match = humor_regex.search(text) if humor_match: print(\"Humor detected!\") else: print(\"No humor detected.\") ``` This code uses the re.search() method to search for the humor regex pattern in the given text. If the pattern is found, the humor_match variable will be set to the match object, otherwise it will be set to None. We then check if the humor_match variable is None, which means that no humor was found. If it is not None, we print a message indicating that humor was detected. Otherwise, we print a message indicating that no humor was detected.\n",
      "\n",
      "Step 2/2\n",
      "Finally, we print the text itself to show that it was not modified in any way. Here's the output: ``` Humor detected! [What's more effective than an islamic call to prayer? a rape whistle.] ``` Therefore, the answer is 'TRUE' if the text contains humor, and 'FALSE' if it does not.<eos>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test the finetuned model on the test set\n",
    "result = classify_humor_tuned2b(test_joke1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0557d-a4c6-45a1-a374-c8dc294f2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify humor for all text in the test set\n",
    "def classify_all_humor_tuned2b(df):\n",
    "    df['predicted_humor'] = df['text'].apply(classify_humor_tuned2b)\n",
    "    return df\n",
    "\n",
    "def process_and_save_tuned(df):\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Apply the function to the test set\n",
    "    df_copy = classify_all_humor_tuned2b(df_copy)\n",
    "\n",
    "    # Convert the 'humor' and 'predicted_humor' columns to boolean\n",
    "    df_copy['humor'] = df_copy['humor'].apply(lambda x: True if x == 'TRUE' else False)\n",
    "    df_copy['predicted_humor'] = df_copy['predicted_humor'].apply(lambda x: True if x == 'TRUE' else False)\n",
    "\n",
    "    # Print the F1 score, confusion matrix, and other relevant classification metrics\n",
    "    classification_report_str = classification_report(df_copy['humor'], df_copy['predicted_humor'])\n",
    "\n",
    "    # Save the classification report\n",
    "    with open('tuned_classification_report.txt', 'w') as f:\n",
    "        f.write(classification_report_str)\n",
    "\n",
    "    # Generate and save the confusion matrix\n",
    "    confusion_mat = confusion_matrix(df_copy['humor'], df_copy['predicted_humor'])\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(confusion_mat, annot=True)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.savefig('tuned_confusion_matrix.png')\n",
    "\n",
    "# Call the function with your dataframe\n",
    "process_and_save_tuned(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
